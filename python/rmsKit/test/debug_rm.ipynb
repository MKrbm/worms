{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/project/python/rmsKit\n"
     ]
    }
   ],
   "source": [
    "cd '/home/user/project/python/rmsKit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "import sys\n",
    "sys.path.append(\"/home/user/project/python/rmsKit\")\n",
    "import rms\n",
    "\n",
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.config import config\n",
    "sys.path.append(\"/home/user/project/python/reduce_nsp\")\n",
    "sys.path.append(\"/home/user/project/python/exact\")\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "groundstate = np.load(\"../exact/test/out/KH_2x2/Jx_1_Jy_1_Jz_1_h_0/groundstate.npy\")\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rms' from '/home/user/project/python/rmsKit/rms/__init__.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rms\n",
    "reload(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnitaryRiemanGenerator is initialized\n"
     ]
    }
   ],
   "source": [
    "ur = rms.unitary.UnitaryRiemanGenerator(8, jax.random.PRNGKey(0), np.float64)\n",
    "u = ur.reset_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/user/project/python/rmsKit\")\n",
    "from jax_lattice import KH\n",
    "\n",
    "p = dict(\n",
    "    Jx=1,\n",
    "    Jy=1,\n",
    "    Jz=1,\n",
    "    hx=0,\n",
    "    hz=0,\n",
    ")\n",
    "H = KH.system([2, 2], \"3site\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "u = np.load(\"array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "# U = np.kron(u,u)\n",
    "# U = np.kron(U,U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = rms.loss.init_loss(jnp.array(H), 8, np.float64, \"sel\", beta=1.0)\n",
    "state_list = [state]\n",
    "qesLoss = rms.loss.system_el_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(u):\n",
    "    return qesLoss(state_list, jnp.array(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit loss(jnp.array(u)).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "v, g = jax.value_and_grad(loss)(jnp.array(u))\n",
    "v.block_until_ready(), g.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/user/project/python/rmsKit\")\n",
    "import os\n",
    "import torch\n",
    "from lattice import KH\n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# # os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import rms_torch\n",
    "import numpy as np\n",
    "# u = np.load(\"/home/user/project/python/rmsKit/array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "u = np.load(\"/home/user/project/python/rmsKit/array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1_e_50_lr_0.5/u/0.npy\")\n",
    "device = torch.device(\"cuda\")\n",
    "p = dict(\n",
    "    Jx=1,\n",
    "    Jy=1,\n",
    "    Jz=1,\n",
    "    hx=0,\n",
    "    hz=0,\n",
    ")\n",
    "H = KH.system([2, 2], \"3site\", p)\n",
    "# Create an instance of the CustomModel class\n",
    "E, V = np.linalg.eigh(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_gpu = torch.tensor(H, dtype=torch.float64, device=device)\n",
    "model = rms_torch.UnitaryRieman(H.shape[0], 3, device=device, u0=u).to(device)\n",
    "loss = rms_torch.SystemQuasiEnergyLoss(H, N = 10, device=device).to(device)\n",
    "compiled_model = torch.compile(model, dynamic = False, fullgraph=True)\n",
    "compiled_loss = torch.compile(loss, dynamic = False, fullgraph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0341, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<CompiledFunctionBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_loss(compiled_model(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.6941, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<CompiledFunctionBackward>)\n",
      "CPU times: user 1.1 s, sys: 0 ns, total: 1.1 s\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = compiled_loss(compiled_model())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3589,  0.5424, -0.2497, -0.2731,  0.0876, -0.3453,  0.4022, -0.3891],\n",
      "        [-0.0693, -0.3173, -0.5213,  0.3710,  0.3618,  0.3694,  0.0897, -0.4578],\n",
      "        [ 0.0471,  0.1124, -0.7220, -0.1243, -0.0719, -0.2347, -0.5561,  0.2811],\n",
      "        [ 0.2508, -0.0900,  0.2653, -0.3463,  0.6088, -0.2487, -0.4199, -0.3603],\n",
      "        [-0.7052, -0.5560,  0.0149, -0.3474,  0.1196, -0.0798,  0.0194,  0.2274],\n",
      "        [ 0.2965, -0.4520, -0.1049,  0.2258, -0.1235, -0.7265,  0.3163, -0.0509],\n",
      "        [-0.0373, -0.2100,  0.0238, -0.2446, -0.6752,  0.0755, -0.2696, -0.5998],\n",
      "        [-0.4634,  0.1638,  0.2499,  0.6487, -0.0064, -0.2958, -0.4142, -0.1271]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "CPU times: user 7.32 ms, sys: 0 ns, total: 7.32 ms\n",
      "Wall time: 6.92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "compiled_model.reset_params()\n",
    "print(next(compiled_model.parameters()).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -9.4834,  -5.2681,   8.1433,  22.2067,   9.3905, -19.0828,  -9.3958,\n",
      "          -5.1588],\n",
      "        [ -6.7860,  -0.5571, -18.1771,  -5.1018,   0.6570,   2.1333,  -1.1790,\n",
      "          -6.4202],\n",
      "        [  1.3660,  13.8845, -14.1871,   6.3950,   2.9877, -22.7653,  20.5231,\n",
      "          -0.9765],\n",
      "        [ -3.5940,  -8.2515,   7.4590, -14.7142,   1.8519, -10.9381,  26.3280,\n",
      "           7.7427],\n",
      "        [ -2.8593,   8.8188,  12.3703, -15.3801, -18.6747,  -1.7176,  14.5213,\n",
      "          -4.7598],\n",
      "        [  7.5711, -24.3100,  16.2202,   4.3194,  -0.5520, -10.4871,   6.9503,\n",
      "          -6.2057],\n",
      "        [  2.7612,   2.9957,  13.2747,  -3.3060, -10.9493, -27.6429,   7.6486,\n",
      "           7.6357],\n",
      "        [ -5.9324,  -0.5958,  15.8760,  14.6059, -21.4196,   8.0603, -19.1400,\n",
      "          -4.7803]], device='cuda:0', dtype=torch.float64)\n",
      "CPU times: user 2.77 s, sys: 9.76 ms, total: 2.78 s\n",
      "Wall time: 2.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "compiled_model.reset_params()\n",
    "x = compiled_loss(compiled_model())\n",
    "x.backward()\n",
    "print(next(compiled_model.parameters()).grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = compiled_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331 µs ± 107 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit compiled_loss(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = torch.tensor(u)\n",
    "# U = torch.kron(u, u)\n",
    "# U = torch.kron(U, U).to(device)\n",
    "# X = torch.from_numpy(V[:,0]).to(device)\n",
    "# SH = loss.stoquastic(loss.H)\n",
    "# SUH = loss.stoquastic(U @ loss.H @ U.T)\n",
    "# SUx = torch.abs(U @ X)\n",
    "# y = SUH @ SUx\n",
    "# y2 = y\n",
    "# for _ in range(10):\n",
    "#     y2 = SUH @ y2 \n",
    "#     y2 /= torch.norm(y2)\n",
    "# quasi_Sgs = torch.abs(y2)\n",
    "# gap = (SUx - quasi_Sgs) @ SUH @ (quasi_Sgs + SUx) \n",
    "# gap - SUx @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.0194, device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<CompiledFunctionBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_loss(compiled_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.57 ms ± 765 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loss(compiled_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4096]], which is output 0 of DivBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2087\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2086\u001b[0m     flattened_joints, _ \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(joint_inputs)\n\u001b[0;32m-> 2087\u001b[0m     fx_g \u001b[39m=\u001b[39m make_fx(joint_forward_backward, aot_config\u001b[39m.\u001b[39;49mdecompositions)(\n\u001b[1;32m   2088\u001b[0m         \u001b[39m*\u001b[39;49mjoint_inputs\n\u001b[1;32m   2089\u001b[0m     )\n\u001b[1;32m   2091\u001b[0m \u001b[39m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:714\u001b[0m, in \u001b[0;36mmake_fx.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mwith\u001b[39;00m decompose(decomposition_table), fake_tensor_mode, python_dispatcher_mode, \\\n\u001b[1;32m    713\u001b[0m      sym_mode, proxy_mode, disable_autocast_cache(), disable_proxy_modes_tracing(enable_current\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 714\u001b[0m     t \u001b[39m=\u001b[39m dispatch_trace(wrap_key(func, args, fx_tracer), tracer\u001b[39m=\u001b[39;49mfx_tracer, concrete_args\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(phs))\n\u001b[1;32m    716\u001b[0m \u001b[39m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:443\u001b[0m, in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_trace\u001b[39m(\n\u001b[1;32m    439\u001b[0m         root: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, Callable],\n\u001b[1;32m    440\u001b[0m         tracer: Tracer,\n\u001b[1;32m    441\u001b[0m         concrete_args: Optional[Tuple[Any, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m GraphModule:\n\u001b[0;32m--> 443\u001b[0m     graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(root, concrete_args)\n\u001b[1;32m    444\u001b[0m     name \u001b[39m=\u001b[39m root\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m root\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m         _autowrap_check(\n\u001b[1;32m    773\u001b[0m             patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    779\u001b[0m         {},\n\u001b[1;32m    780\u001b[0m         type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m     )\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py:652\u001b[0m, in \u001b[0;36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    651\u001b[0m tree_args \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_unflatten(\u001b[39mlist\u001b[39m(args), in_spec)\n\u001b[0;32m--> 652\u001b[0m tree_out \u001b[39m=\u001b[39m root_fn(\u001b[39m*\u001b[39;49mtree_args)\n\u001b[1;32m    653\u001b[0m out_args, out_spec \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_flatten(tree_out)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/fx/experimental/proxy_tensor.py:459\u001b[0m, in \u001b[0;36mwrap_key.<locals>.wrapped\u001b[0;34m(*proxies)\u001b[0m\n\u001b[1;32m    457\u001b[0m     track_tensor_tree(flat_tensors, flat_proxies, constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tracer\u001b[39m=\u001b[39mtracer)\n\u001b[0;32m--> 459\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m    460\u001b[0m out \u001b[39m=\u001b[39m pytree\u001b[39m.\u001b[39mtree_map_only(\n\u001b[1;32m    461\u001b[0m     torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    462\u001b[0m     \u001b[39mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mproxy),\n\u001b[1;32m    463\u001b[0m     out\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1156\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.traced_joint\u001b[0;34m(primals, tangents)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraced_joint\u001b[39m(primals, tangents):\n\u001b[0;32m-> 1156\u001b[0m     \u001b[39mreturn\u001b[39;00m functionalized_f_helper(primals, tangents)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1108\u001b[0m, in \u001b[0;36mcreate_forward_or_joint_functionalized.<locals>.functionalized_f_helper\u001b[0;34m(primals, maybe_tangents)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[39m# Run the joint\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m     f_outs \u001b[39m=\u001b[39m flat_fn_no_input_mutations(fn, f_primals, f_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1109\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1076\u001b[0m, in \u001b[0;36mflat_fn_no_input_mutations\u001b[0;34m(fn, primals, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     primals_after_cloning \u001b[39m=\u001b[39m primals\n\u001b[0;32m-> 1076\u001b[0m outs \u001b[39m=\u001b[39m flat_fn_with_synthetic_bases_expanded(fn, primals, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1077\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1048\u001b[0m, in \u001b[0;36mflat_fn_with_synthetic_bases_expanded\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(meta\u001b[39m.\u001b[39mfw_metadata\u001b[39m.\u001b[39minput_info) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(primals)\n\u001b[0;32m-> 1048\u001b[0m outs \u001b[39m=\u001b[39m forward_or_joint(fn, primals_before_cloning, primals, maybe_tangents, meta, keep_input_mutations)\n\u001b[1;32m   1049\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1017\u001b[0m, in \u001b[0;36mforward_or_joint\u001b[0;34m(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[39mwith\u001b[39;00m fx_traceback\u001b[39m.\u001b[39mpreserve_node_meta():\n\u001b[0;32m-> 1017\u001b[0m         backward_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(\n\u001b[1;32m   1018\u001b[0m             needed_outs,\n\u001b[1;32m   1019\u001b[0m             grad_primals,\n\u001b[1;32m   1020\u001b[0m             grad_outputs\u001b[39m=\u001b[39;49mneeded_tangents,\n\u001b[1;32m   1021\u001b[0m             allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1022\u001b[0m         )\n\u001b[1;32m   1023\u001b[0m backward_out_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(backward_out)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/__init__.py:269\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    270\u001b[0m         grad,\n\u001b[1;32m    271\u001b[0m         overridable_args,\n\u001b[1;32m    272\u001b[0m         t_outputs,\n\u001b[1;32m    273\u001b[0m         t_inputs,\n\u001b[1;32m    274\u001b[0m         grad_outputs\u001b[39m=\u001b[39;49mgrad_outputs,\n\u001b[1;32m    275\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    276\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    277\u001b[0m         only_inputs\u001b[39m=\u001b[39;49monly_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused\u001b[39m=\u001b[39;49mallow_unused,\n\u001b[1;32m    279\u001b[0m         is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_inputs:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m     result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_inductor/overrides.py:38\u001b[0m, in \u001b[0;36mAutogradMonkeypatch.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m replacements[func](\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4096]], which is output 0 of DivBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mtimeit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcompiled_loss(compiled_model())\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2369\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2368\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2369\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2371\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/IPython/core/magics/execution.py:1163\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m   1162\u001b[0m     number \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m index\n\u001b[0;32m-> 1163\u001b[0m     time_number \u001b[39m=\u001b[39m timer\u001b[39m.\u001b[39;49mtimeit(number)\n\u001b[1;32m   1164\u001b[0m     \u001b[39mif\u001b[39;00m time_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m:\n\u001b[1;32m   1165\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/IPython/core/magics/execution.py:157\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    155\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    156\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    158\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.DoubleTensor [4096]], which is output 0 of DivBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "%timeit compiled_loss(compiled_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 23.314299400009702\n",
      "Epoch: 1, Loss: 23.038843048493845\n",
      "Epoch: 2, Loss: 22.755859174870388\n",
      "Epoch: 3, Loss: 22.463636181197213\n",
      "Epoch: 4, Loss: 22.169798842247978\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m grad \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mgrad  \u001b[39m# Get the gradient from the compiled model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m grad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     grad\u001b[39m.\u001b[39mdata[:] \u001b[39m=\u001b[39m rms_torch\u001b[39m.\u001b[39;49mriemannian_grad_torch(p\u001b[39m.\u001b[39;49mdata, grad)\n\u001b[1;32m     16\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo gradient for parameter\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/project/python/rmsKit/rms_torch/functions.py:14\u001b[0m, in \u001b[0;36mriemannian_grad_torch\u001b[0;34m(u, euc_grad)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mriemannian_grad_torch\u001b[39m(u: torch\u001b[39m.\u001b[39mTensor, euc_grad: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_is_unitary_torch(u):\n\u001b[1;32m     15\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mu must be unitary matrix\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m u\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m euc_grad\u001b[39m.\u001b[39mdtype:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = rms_torch.Adam(compiled_model.parameters(), lr=1e-3, amsgrad=True)\n",
    "optimizer = rms_torch.LION(compiled_model.parameters(), lr=1*1e-2)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = compiled_model()\n",
    "    loss = compiled_loss(output)\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        grad = p.grad  # Get the gradient from the compiled model\n",
    "        if grad is not None:\n",
    "            grad.data[:] = rms_torch.riemannian_grad_torch(p.data, grad)\n",
    "        else:\n",
    "            raise RuntimeError(\"No gradient for parameter\")\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.5696e-01,  1.7097e-01,  1.9867e-01, -1.2842e-01, -1.0919e-01,\n",
       "          6.8600e-01, -2.0272e-01, -5.7023e-01],\n",
       "        [-3.8232e-02, -5.4888e-01,  1.9174e-01,  4.7085e-01,  2.6820e-01,\n",
       "          4.8821e-01, -2.3459e-02,  3.5774e-01],\n",
       "        [-3.5027e-01, -3.8501e-01, -1.0177e-01, -1.7628e-01, -3.0537e-01,\n",
       "          3.9967e-02,  7.6094e-01, -1.1732e-01],\n",
       "        [-8.9114e-02, -2.2356e-01, -7.4985e-01, -4.1507e-01,  3.4657e-01,\n",
       "          1.9739e-01, -2.1377e-01,  5.2455e-02],\n",
       "        [ 1.7096e-01, -1.5171e-01,  5.6047e-01, -7.0626e-01,  3.2317e-01,\n",
       "          1.1962e-02,  6.7080e-02,  1.6045e-01],\n",
       "        [-5.7808e-01, -3.9429e-01,  1.8568e-01,  3.4663e-02,  1.0437e-01,\n",
       "         -4.8289e-01, -4.0521e-01, -2.5770e-01],\n",
       "        [ 1.4227e-01, -3.0032e-01, -1.6723e-04, -2.2395e-01, -7.6582e-01,\n",
       "          8.1621e-02, -4.0599e-01,  2.8538e-01],\n",
       "        [-6.4671e-01,  4.5012e-01,  5.0319e-02, -9.1091e-02, -3.1170e-02,\n",
       "          1.0248e-01, -1.0116e-02,  5.9729e-01]], device='cuda:0',\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No gradient for parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m         grad\u001b[39m.\u001b[39mdata[:] \u001b[39m=\u001b[39m rms_torch\u001b[39m.\u001b[39mriemannian_grad_torch(p\u001b[39m.\u001b[39mdata, grad)\n\u001b[1;32m     16\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo gradient for parameter\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No gradient for parameter"
     ]
    }
   ],
   "source": [
    "compiled_model.reset_params()\n",
    "optimizer = rms_torch.LION(compiled_model.parameters(), lr=3*1e-4)\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = compiled_model()\n",
    "    loss = compiled_loss(output)\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        grad = p.grad  # Get the gradient from the compiled model\n",
    "        if grad is not None:\n",
    "            grad.data[:] = rms_torch.riemannian_grad_torch(p.data, grad)\n",
    "        else:\n",
    "            raise RuntimeError(\"No gradient for parameter\")\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "import sys\n",
    "sys.path.append(\"/home/user/project/python/rmsKit\")\n",
    "import rms\n",
    "\n",
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "sys.path.append(\"/home/user/project/python/reduce_nsp\")\n",
    "sys.path.append(\"/home/user/project/python/exact\")\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "groundstate = np.load(\"/home/user/project/python/exact/test/out/KH_2x2/Jx_1_Jy_1_Jz_1_h_0/groundstate.npy\")\n",
    "u = np.load(\"/home/user/project/python/rmsKit/array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "H0 = -np.load(\"/home/user/project/python/rmsKit/array/KH/3site/none/Jx_1_Jy_1_Jz_1_hx_0_hz_0/H/0.npy\")\n",
    "H1 = -np.load(\"/home/user/project/python/rmsKit/array/KH/3site/none/Jx_1_Jy_1_Jz_1_hx_0_hz_0/H/1.npy\")\n",
    "H2 = -np.load(\"/home/user/project/python/rmsKit/array/KH/3site/none/Jx_1_Jy_1_Jz_1_hx_0_hz_0/H/2.npy\")\n",
    "x1 = groundstate.copy()\n",
    "x0 = groundstate.reshape([8] * 4)\n",
    "x0 = x0.transpose([0, 2, 1, 3]).reshape(-1)\n",
    "x0 = jnp.array(x0)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "SH0 = rms.sum_ham(H0, [[0,2]], 4, 8)\n",
    "SH1 = rms.sum_ham(H1, [[0,1]], 4, 8)\n",
    "SH2 = rms.sum_ham(H2, [[0,3]], 4, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bonds = [[0,0,2], [1,0,1], [2,0,3], [0,1,3], [1,1,0], [2,1,2], [0,2,0], [1,2,3], \n",
    "            [2,2,1], [0,3,1], [1,3,2], [2,3,0]]\n",
    "bonds = [[], [], []]\n",
    "for bond in _bonds:\n",
    "    bonds[bond[0]].append(bond[1:])\n",
    "\n",
    "_H = rms.sum_ham(rms.stoquastic(H0), bonds[0], 4, 8)\n",
    "_H += rms.sum_ham(rms.stoquastic(H1), bonds[1], 4, 8)\n",
    "_H += rms.sum_ham(rms.stoquastic(H2), bonds[2], 4, 8)\n",
    "\n",
    "u2 = np.kron(u, u)\n",
    "_HU = rms.sum_ham(rms.stoquastic(u2@H0@u2.T), bonds[0], 4, 8)\n",
    "_HU += rms.sum_ham(rms.stoquastic(u2@H1@u2.T), bonds[1], 4, 8)\n",
    "_HU += rms.sum_ham(rms.stoquastic(u2@H2@u2.T), bonds[2], 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.052132213843223\n",
      "-6.22515992356388\n"
     ]
    }
   ],
   "source": [
    "U = np.kron(u,u)\n",
    "U = np.kron(U,U)\n",
    "\n",
    "SH = SH0 + SH1 + SH2\n",
    "\n",
    "x = jnp.abs(U @ groundstate)\n",
    "print(x @ _HU @ x)\n",
    "x = jnp.abs(groundstate)\n",
    "print(x @ rms.stoquastic(H) @ x)\n",
    "# print(groundstate @ rms.stoquastic(H) @ groundstate)\n",
    "\n",
    "# jnp.linalg.eigvalsh(rms.stoquastic(U @ H @ U.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.052130856520556"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.763032714130139 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.579816545884742e-14"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(_H - rms.stoquastic(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-8.19503796, -7.8031308 , -7.8031308 , ...,  5.        ,\n",
       "        6.        ,  6.        ], dtype=float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.eigvalsh(rms.stoquastic(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-8.19503796, -7.8031308 , -7.8031308 , ...,  5.        ,\n",
       "        6.        ,  6.        ], dtype=float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.eigvalsh(_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-5.44487522, -5.3283924 , -5.29823654, ...,  6.        ,\n",
       "        6.        ,  6.        ], dtype=float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.eigvalsh(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.52065724, dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = rms.loss.init_loss(jnp.array(H1), 8, np.float64, \"qes\", X=jnp.array(x1))\n",
    "qesLoss = rms.loss.qes_multi\n",
    "\n",
    "qesLoss([state], jnp.array(u))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "u = np.load(\"/home/user/project/python/rmsKit/array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def random_unitary_matrix(size, device):\n",
    "    random_matrix = np.random.randn(size, size)\n",
    "    q, _ = np.linalg.qr(random_matrix)\n",
    "    return torch.tensor(u, dtype=torch.float64, device=device)\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, H_size, unitary_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Calculate the number of unitary matrices required to match the size of 'H'\n",
    "        n_us = round(math.log2(H_size) / math.log2(unitary_size))\n",
    "        self.n_us = n_us\n",
    "\n",
    "        # Initialize the given number of random unitary matrices\n",
    "        self.us = nn.ParameterList([nn.Parameter(random_unitary_matrix(unitary_size, device), requires_grad=True) for _ in range(n_us)])\n",
    "\n",
    "    def forward(self):\n",
    "        # Calculate U as the Kronecker product of all the unitary matrices in 'us'\n",
    "        U = self.us[0]\n",
    "        U = torch.kron(U, self.us[1])\n",
    "        U = torch.kron(U, self.us[2])\n",
    "        U = torch.kron(U, self.us[3])\n",
    "        # for i in range(4):\n",
    "        #     U = torch.kron(U, self.us[i+1])\n",
    "        # for u in self.us[1:]:\n",
    "        #     U = torch.kron(U, u)\n",
    "        return U\n",
    "\n",
    "def custom_loss(U, H):\n",
    "    # Calculate U @ H @ U.T\n",
    "    A = torch.matmul(U, torch.matmul(H, U.T))\n",
    "\n",
    "    # Calculate the absolute value of the result\n",
    "    a = torch.max(torch.diag(A)) * torch.eye(A.shape[0], device=device)\n",
    "    result_abs = -torch.abs(A - a) + a\n",
    "\n",
    "    # Calculate the minimum eigenvalue\n",
    "    E = torch.linalg.eigvalsh(result_abs)\n",
    "    z = torch.exp(-E * 1).sum()\n",
    "    return torch.log(z)\n",
    "\n",
    "p = dict(\n",
    "    Jx=1,\n",
    "    Jy=1,\n",
    "    Jz=1,\n",
    "    hx=0,\n",
    "    hz=0,\n",
    ")\n",
    "H = KH.system([2, 2], \"3site\", p)\n",
    "\n",
    "# Create an instance of the CustomModel class\n",
    "model = CustomModel(H.shape[0], 8).to(device)\n",
    "compiled_model = torch.compile(model, dynamic = False, fullgraph=True)\n",
    "# Calculate the output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "635093cb382d24e7bb09df67eef84e97b3e0429c00b0294b3c9882ac411b8a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
