{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/user/project/python/rmsKit\")\n",
    "from jax_lattice import KH\n",
    "\n",
    "p = dict(\n",
    "    Jx=1,\n",
    "    Jy=1,\n",
    "    Jz=1,\n",
    "    hx=0,\n",
    "    hz=0,\n",
    ")\n",
    "H = KH.system([2, 2], \"3site\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "u = np.load(\"array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "# U = np.kron(u,u)\n",
    "# U = np.kron(U,U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = rms.loss.init_loss(jnp.array(H), 8, np.float64, \"sel\", beta=1.0)\n",
    "state_list = [state]\n",
    "qesLoss = rms.loss.system_el_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(u):\n",
    "    return qesLoss(state_list, jnp.array(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit loss(jnp.array(u)).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "v, g = jax.value_and_grad(loss)(jnp.array(u))\n",
    "v.block_until_ready(), g.block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/home/user/project/python/rmsKit\")\n",
    "import os\n",
    "import torch\n",
    "from lattice import KH\n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# # os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "import rms_torch\n",
    "import numpy as np\n",
    "# u = np.load(\"/home/user/project/python/rmsKit/array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "u = np.load(\"/home/user/project/python/rmsKit/array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1_e_50_lr_0.5/u/0.npy\")\n",
    "device = torch.device(\"cuda\")\n",
    "p = dict(\n",
    "    Jx=1,\n",
    "    Jy=1,\n",
    "    Jz=1,\n",
    "    hx=0,\n",
    "    hz=0,\n",
    ")\n",
    "H = KH.system([2, 2], \"3site\", p)\n",
    "# Create an instance of the CustomModel class\n",
    "E, V = np.linalg.eigh(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m H_gpu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(H, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m rms_torch\u001b[39m.\u001b[39mUnitaryRieman(H\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m8\u001b[39m, device\u001b[39m=\u001b[39mdevice, u0\u001b[39m=\u001b[39mu)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m loss \u001b[39m=\u001b[39m rms_torch\u001b[39m.\u001b[39;49mSystemMinimumEnergyLoss(H, device\u001b[39m=\u001b[39;49mdevice)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m compiled_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(model, dynamic \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, fullgraph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m compiled_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(loss, dynamic \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, fullgraph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/project/python/rmsKit/rms_torch/loss.py:68\u001b[0m, in \u001b[0;36mSystemMinimumEnergyLoss.__init__\u001b[0;34m(self, H, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, H: Union[np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor], device: torch\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m---> 68\u001b[0m     \u001b[39msuper\u001b[39;49m(SystemMinimumEnergyLoss, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(H, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m     71\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(H)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "H_gpu = torch.tensor(H, dtype=torch.float64, device=device)\n",
    "model = rms_torch.UnitaryRieman(H.shape[0], 8, device=device, u0=u).to(device)\n",
    "loss = rms_torch.SystemMinimumEnergyLoss(H, device=device).to(device)\n",
    "compiled_model = torch.compile(model, dynamic = False, fullgraph=True)\n",
    "compiled_loss = torch.compile(loss, dynamic = False, fullgraph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4096x4096 and 81x81)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m rms_torch\u001b[39m.\u001b[39mLION(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\u001b[39m*\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[1;32m      3\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> 4\u001b[0m loss\u001b[39m.\u001b[39;49minitialize(model())\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Zero out the gradients\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/project/python/rmsKit/rms_torch/loss.py:101\u001b[0m, in \u001b[0;36mSystemMinimumEnergyLoss.initialize\u001b[0;34m(self, U)\u001b[0m\n\u001b[1;32m     99\u001b[0m     U \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meye(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    100\u001b[0m U \u001b[39m=\u001b[39m U\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m--> 101\u001b[0m A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(U, torch\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mH, U\u001b[39m.\u001b[39;49mT))\n\u001b[1;32m    102\u001b[0m H_tilde \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstoquastic(A)\n\u001b[1;32m    103\u001b[0m E, V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigh(H_tilde)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4096x4096 and 81x81)"
     ]
    }
   ],
   "source": [
    "# optimizer = rms_torch.Adam(compiled_model.parameters(), lr=1e-3, amsgrad=True)\n",
    "optimizer = rms_torch.LION(model.parameters(), lr=1*1e-3)\n",
    "num_epochs = 10\n",
    "loss.initialize(model())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model()\n",
    "    _loss = loss(output)\n",
    "    _loss.backward()\n",
    "    for p in model.parameters():\n",
    "        grad = p.grad  # Get the gradient from the compiled model\n",
    "        if grad is not None:\n",
    "            grad.data[:] = rms_torch.riemannian_grad_torch(p.data, grad)\n",
    "        else:\n",
    "            raise RuntimeError(\"No gradient for parameter\")\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_loss.V_old.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_model().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(z+2) if True else (z+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.161143\n",
      "../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.197812\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def list_unitaries(path, n_percent=None, n_top=None, thres=None):\n",
    "    loss_folders = [\n",
    "        entry.path for entry in os.scandir(path) if entry.is_dir() and entry.name.startswith(\"loss_\")\n",
    "    ]\n",
    "\n",
    "    def get_folder_number(folder_name):\n",
    "        match = re.search(r\"loss_(\\d+\\.\\d+)\", folder_name)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        return float(\"inf\")\n",
    "\n",
    "    loss_folders.sort(key=get_folder_number)\n",
    "    selected_folders = [loss_folders for _ in range(3)]\n",
    "\n",
    "    if n_percent is not None:\n",
    "        if isinstance(n_percent, int) and 0 <= n_percent <= 100:\n",
    "            num_folders = int(len(loss_folders) * n_percent / 100)\n",
    "            selected_folders[0] = loss_folders[:num_folders]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid n_percent value. Please enter an integer value between 0 and 100.\")\n",
    "\n",
    "    if n_top is not None:\n",
    "        if isinstance(n_top, int) and 0 <= n_top <= len(loss_folders):\n",
    "            selected_folders[1] = loss_folders[:n_top]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid n_top value. Please enter an integer value between 0 and the total number of folders.\")\n",
    "\n",
    "    if thres is not None:\n",
    "        if isinstance(thres, (int, float)):\n",
    "            selected_folders[2] = [folder for folder in loss_folders if get_folder_number(folder) < thres]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid threshold value. Please enter a valid number.\")\n",
    "\n",
    "    # 3つのリストのうち要素数が最も少ないものを選択\n",
    "    result = min(selected_folders, key=len)\n",
    "    return result\n",
    "\n",
    "path = \"../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5\"  # ディレクトリのパスを指定してください\n",
    "loss_folders = list_unitaries(path, n_top=10, thres=8.2)\n",
    "\n",
    "for folder in loss_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.977042',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.899173',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.791781',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.784688',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.730649',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.598964',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.577018',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.489171',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.450588',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.365743',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.275466',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.271066',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.261912',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.191866',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.189719',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.171665',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.081785',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_9.068416',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.965562',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.958275',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.942443',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.936326',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.934967',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.917678',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.857450',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.835845',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.705478',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.691806',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.671092',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.669106',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.645495',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.606419',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.593450',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.589636',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.573960',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.555533',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.551776',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.550237',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.544872',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.532559',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.531009',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.523286',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.516493',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.512897',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.511711',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.506212',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.504619',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.500255',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.500245',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.498080',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.496029',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.495646',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.495200',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.494950',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.494824',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.493936',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.491135',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.490715',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.490622',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.485745',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.484326',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.483839',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.482978',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.482950',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.481048',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.473473',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.472696',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.472679',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.472517',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.471863',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.471332',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.471291',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.469788',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.468634',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.468550',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.467921',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.467106',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.462990',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.462975',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.461182',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.459181',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.458581',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.458330',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.454806',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.451410',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.449738',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.445944',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.443938',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.443268',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.436657',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.434198',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.434023',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.433774',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.433584',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.433329',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.430870',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.430319',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.430274',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.429146',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.429057',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.428092',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.426351',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.425069',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.424566',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.424042',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.423877',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.423104',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.422047',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.421574',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.416227',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.414898',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.414258',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.412459',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.412290',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.411720',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.411267',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.411097',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.410815',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.410325',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.408557',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.406558',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.403619',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.403249',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.401516',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.401253',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.400760',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.400447',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.399327',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.399034',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.396505',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.395585',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.394932',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.393571',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.392557',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.390075',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.388830',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.388634',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.386624',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.386278',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.384987',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.383543',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.383097',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.382146',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.381595',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.381346',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.375343',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.372863',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.371088',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.370778',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.370717',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.369222',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.368942',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.368857',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.368436',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.366451',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.366055',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.355085',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.353735',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.353367',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.349092',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.341403',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.336170',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.316851',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.314183',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.309431',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.301737',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.297024',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.287461',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.258731',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.257471',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.253946',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.249388',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.236210',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.222577',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.209051',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.206533',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.197812',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_8.161143',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_12.141636',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_11.201482',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_11.129121',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.812261',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.784816',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.747557',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.736427',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.730080',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.659223',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.643863',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.616956',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.566043',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.490894',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.438945',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.367714',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.341086',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.338698',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.286015',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.261287',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.161978',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.109590',\n",
       " '../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5/loss_10.081562']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = list_unitaries(\"../array/torch/KH/3site/sqel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/e_30_lr_0.5\")\n",
    "sorted(li)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 8.004105424341045\n",
      "Epoch: 1, Loss: 8.003236965858935\n",
      "Epoch: 2, Loss: 8.003438837389412\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m output \u001b[39m=\u001b[39m compiled_model()\n\u001b[0;32m---> 12\u001b[0m _loss \u001b[39m=\u001b[39m compiled_loss(output)\n\u001b[1;32m     13\u001b[0m _loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m compiled_model\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/project/python/rmsKit/rms_torch/loss.py:84\u001b[0m, in \u001b[0;36mSystemMinimumEnergyLoss.forward\u001b[0;34m(self, U)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV_old \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m#* V_tilde\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mE_old \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m#* E_min_tilde\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, U: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     85\u001b[0m     A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(U, torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH, U\u001b[39m.\u001b[39mT))\n\u001b[1;32m     86\u001b[0m     H_tilde \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstoquastic(A)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2819\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   2817\u001b[0m full_args\u001b[39m.\u001b[39mextend(params_flat)\n\u001b[1;32m   2818\u001b[0m full_args\u001b[39m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 2819\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1222\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg\u001b[39m(args):\n\u001b[0;32m-> 1222\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2386\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.debug_compiled_function\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m can_require_grad:\n\u001b[1;32m   2381\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m a\u001b[39m.\u001b[39mrequires_grad, format_guard_bug_msg(\n\u001b[1;32m   2382\u001b[0m             aot_config,\n\u001b[1;32m   2383\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdescribe_input(i, aot_config)\u001b[39m}\u001b[39;00m\u001b[39m would not require grad\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2384\u001b[0m         )\n\u001b[0;32m-> 2386\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_function(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1898\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     args_with_synthetic_bases \u001b[39m=\u001b[39m args\n\u001b[1;32m   1897\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39m_force_original_view_tracking(\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m-> 1898\u001b[0m     all_outs \u001b[39m=\u001b[39m call_func_with_args(\n\u001b[1;32m   1899\u001b[0m         compiled_fn,\n\u001b[1;32m   1900\u001b[0m         args_with_synthetic_bases,\n\u001b[1;32m   1901\u001b[0m         disable_amp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1902\u001b[0m     )\n\u001b[1;32m   1904\u001b[0m num_mutated_inps \u001b[39m=\u001b[39m runtime_metadata\u001b[39m.\u001b[39mnum_mutated_inputs\n\u001b[1;32m   1905\u001b[0m num_metadata_mutated_inps \u001b[39m=\u001b[39m runtime_metadata\u001b[39m.\u001b[39mnum_mutated_metadata_inputs\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1247\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1246\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1247\u001b[0m         out \u001b[39m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m   1248\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m         \u001b[39m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m         \u001b[39m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYour compiler for AOTAutograd is returning a a function that doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt take boxed arguments. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1253\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1254\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1222\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg\u001b[39m(args):\n\u001b[0;32m-> 1222\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:2151\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   2143\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m   2144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, \u001b[39m*\u001b[39mdeduped_flat_tensor_args):\n\u001b[1;32m   2145\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     \u001b[39m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m     \u001b[39m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m-> 2151\u001b[0m     fw_outs \u001b[39m=\u001b[39m call_func_with_args(\n\u001b[1;32m   2152\u001b[0m         CompiledFunction\u001b[39m.\u001b[39;49mcompiled_fw,\n\u001b[1;32m   2153\u001b[0m         deduped_flat_tensor_args,\n\u001b[1;32m   2154\u001b[0m         disable_amp\u001b[39m=\u001b[39;49mdisable_amp,\n\u001b[1;32m   2155\u001b[0m     )\n\u001b[1;32m   2157\u001b[0m     num_outputs \u001b[39m=\u001b[39m CompiledFunction\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mnum_outputs\n\u001b[1;32m   2158\u001b[0m     num_outputs_aliased_to_inputs \u001b[39m=\u001b[39m (\n\u001b[1;32m   2159\u001b[0m         CompiledFunction\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mnum_outputs_aliased_to_inputs\n\u001b[1;32m   2160\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py:1247\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1246\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1247\u001b[0m         out \u001b[39m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m   1248\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m         \u001b[39m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m         \u001b[39m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYour compiler for AOTAutograd is returning a a function that doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt take boxed arguments. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1253\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1254\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:248\u001b[0m, in \u001b[0;36malign_inputs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m new_inputs[i]\u001b[39m.\u001b[39mdata_ptr() \u001b[39m%\u001b[39m ALIGNMENT:\n\u001b[1;32m    247\u001b[0m         new_inputs[i] \u001b[39m=\u001b[39m clone_preserve_strides(new_inputs[i])\n\u001b[0;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m model(new_inputs)\n",
      "File \u001b[0;32m/tmp/torchinductor_user/gf/cgfaxpw2x65almiqczsyfqnoegk24vyuf627ektfpzeclz2lktmi.py:165\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdel\u001b[39;00m primals_6\n\u001b[1;32m    164\u001b[0m \u001b[39mdel\u001b[39;00m primals_8\n\u001b[0;32m--> 165\u001b[0m buf4 \u001b[39m=\u001b[39m aten\u001b[39m.\u001b[39;49m_linalg_solve_ex(buf3, primals_9)\n\u001b[1;32m    166\u001b[0m \u001b[39mdel\u001b[39;00m buf3\n\u001b[1;32m    167\u001b[0m \u001b[39mdel\u001b[39;00m primals_9\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = rms_torch.Adam(compiled_model.parameters(), lr=1e-3, amsgrad=True)\n",
    "optimizer = rms_torch.LION(compiled_model.parameters(), lr=1*1e-3)\n",
    "num_epochs = 10\n",
    "compiled_loss.reset(compiled_model())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = compiled_model()\n",
    "    _loss = compiled_loss(output)\n",
    "    _loss.backward(retain_graph=True)\n",
    "    for p in compiled_model.parameters():\n",
    "        grad = p.grad  # Get the gradient from the compiled model\n",
    "        if grad is not None:\n",
    "            grad.data[:] = rms_torch.riemannian_grad_torch(p.data, grad)\n",
    "        else:\n",
    "            raise RuntimeError(\"No gradient for parameter\")\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.5696e-01,  1.7097e-01,  1.9867e-01, -1.2842e-01, -1.0919e-01,\n",
       "          6.8600e-01, -2.0272e-01, -5.7023e-01],\n",
       "        [-3.8232e-02, -5.4888e-01,  1.9174e-01,  4.7085e-01,  2.6820e-01,\n",
       "          4.8821e-01, -2.3459e-02,  3.5774e-01],\n",
       "        [-3.5027e-01, -3.8501e-01, -1.0177e-01, -1.7628e-01, -3.0537e-01,\n",
       "          3.9967e-02,  7.6094e-01, -1.1732e-01],\n",
       "        [-8.9114e-02, -2.2356e-01, -7.4985e-01, -4.1507e-01,  3.4657e-01,\n",
       "          1.9739e-01, -2.1377e-01,  5.2455e-02],\n",
       "        [ 1.7096e-01, -1.5171e-01,  5.6047e-01, -7.0626e-01,  3.2317e-01,\n",
       "          1.1962e-02,  6.7080e-02,  1.6045e-01],\n",
       "        [-5.7808e-01, -3.9429e-01,  1.8568e-01,  3.4663e-02,  1.0437e-01,\n",
       "         -4.8289e-01, -4.0521e-01, -2.5770e-01],\n",
       "        [ 1.4227e-01, -3.0032e-01, -1.6723e-04, -2.2395e-01, -7.6582e-01,\n",
       "          8.1621e-02, -4.0599e-01,  2.8538e-01],\n",
       "        [-6.4671e-01,  4.5012e-01,  5.0319e-02, -9.1091e-02, -3.1170e-02,\n",
       "          1.0248e-01, -1.0116e-02,  5.9729e-01]], device='cuda:0',\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No gradient for parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m         grad\u001b[39m.\u001b[39mdata[:] \u001b[39m=\u001b[39m rms_torch\u001b[39m.\u001b[39mriemannian_grad_torch(p\u001b[39m.\u001b[39mdata, grad)\n\u001b[1;32m     16\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo gradient for parameter\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No gradient for parameter"
     ]
    }
   ],
   "source": [
    "compiled_model.reset_params()\n",
    "optimizer = rms_torch.LION(compiled_model.parameters(), lr=3*1e-4)\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = compiled_model()\n",
    "    loss = compiled_loss(output)\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        grad = p.grad  # Get the gradient from the compiled model\n",
    "        if grad is not None:\n",
    "            grad.data[:] = rms_torch.riemannian_grad_torch(p.data, grad)\n",
    "        else:\n",
    "            raise RuntimeError(\"No gradient for parameter\")\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "import sys\n",
    "sys.path.append(\"/home/user/project/python/rmsKit\")\n",
    "import rms\n",
    "\n",
    "import jax \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "sys.path.append(\"/home/user/project/python/reduce_nsp\")\n",
    "sys.path.append(\"/home/user/project/python/exact\")\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "groundstate = np.load(\"/home/user/project/python/exact/test/out/KH_2x2/Jx_1_Jy_1_Jz_1_h_0/groundstate.npy\")\n",
    "u = np.load(\"/home/user/project/python/rmsKit/array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "H0 = -np.load(\"/home/user/project/python/rmsKit/array/KH/3site/none/Jx_1_Jy_1_Jz_1_hx_0_hz_0/H/0.npy\")\n",
    "H1 = -np.load(\"/home/user/project/python/rmsKit/array/KH/3site/none/Jx_1_Jy_1_Jz_1_hx_0_hz_0/H/1.npy\")\n",
    "H2 = -np.load(\"/home/user/project/python/rmsKit/array/KH/3site/none/Jx_1_Jy_1_Jz_1_hx_0_hz_0/H/2.npy\")\n",
    "x1 = groundstate.copy()\n",
    "x0 = groundstate.reshape([8] * 4)\n",
    "x0 = x0.transpose([0, 2, 1, 3]).reshape(-1)\n",
    "x0 = jnp.array(x0)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "SH0 = rms.sum_ham(H0, [[0,2]], 4, 8)\n",
    "SH1 = rms.sum_ham(H1, [[0,1]], 4, 8)\n",
    "SH2 = rms.sum_ham(H2, [[0,3]], 4, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bonds = [[0,0,2], [1,0,1], [2,0,3], [0,1,3], [1,1,0], [2,1,2], [0,2,0], [1,2,3], \n",
    "            [2,2,1], [0,3,1], [1,3,2], [2,3,0]]\n",
    "bonds = [[], [], []]\n",
    "for bond in _bonds:\n",
    "    bonds[bond[0]].append(bond[1:])\n",
    "\n",
    "_H = rms.sum_ham(rms.stoquastic(H0), bonds[0], 4, 8)\n",
    "_H += rms.sum_ham(rms.stoquastic(H1), bonds[1], 4, 8)\n",
    "_H += rms.sum_ham(rms.stoquastic(H2), bonds[2], 4, 8)\n",
    "\n",
    "u2 = np.kron(u, u)\n",
    "_HU = rms.sum_ham(rms.stoquastic(u2@H0@u2.T), bonds[0], 4, 8)\n",
    "_HU += rms.sum_ham(rms.stoquastic(u2@H1@u2.T), bonds[1], 4, 8)\n",
    "_HU += rms.sum_ham(rms.stoquastic(u2@H2@u2.T), bonds[2], 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.052132213843223\n",
      "-6.22515992356388\n"
     ]
    }
   ],
   "source": [
    "U = np.kron(u,u)\n",
    "U = np.kron(U,U)\n",
    "\n",
    "SH = SH0 + SH1 + SH2\n",
    "\n",
    "x = jnp.abs(U @ groundstate)\n",
    "print(x @ _HU @ x)\n",
    "x = jnp.abs(groundstate)\n",
    "print(x @ rms.stoquastic(H) @ x)\n",
    "# print(groundstate @ rms.stoquastic(H) @ groundstate)\n",
    "\n",
    "# jnp.linalg.eigvalsh(rms.stoquastic(U @ H @ U.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.052130856520556"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.763032714130139 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.579816545884742e-14"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(_H - rms.stoquastic(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-8.19503796, -7.8031308 , -7.8031308 , ...,  5.        ,\n",
       "        6.        ,  6.        ], dtype=float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.eigvalsh(rms.stoquastic(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-8.19503796, -7.8031308 , -7.8031308 , ...,  5.        ,\n",
       "        6.        ,  6.        ], dtype=float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.eigvalsh(_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-5.44487522, -5.3283924 , -5.29823654, ...,  6.        ,\n",
       "        6.        ,  6.        ], dtype=float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.eigvalsh(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.52065724, dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = rms.loss.init_loss(jnp.array(H1), 8, np.float64, \"qes\", X=jnp.array(x1))\n",
    "qesLoss = rms.loss.qes_multi\n",
    "\n",
    "qesLoss([state], jnp.array(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "u = np.load(\"/home/user/project/python/rmsKit/array/KH/3site/sel/Jx_1_Jy_1_Jz_1_hx_0_hz_0/M_1/u/0.npy\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def random_unitary_matrix(size, device):\n",
    "    random_matrix = np.random.randn(size, size)\n",
    "    q, _ = np.linalg.qr(random_matrix)\n",
    "    return torch.tensor(u, dtype=torch.float64, device=device)\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, H_size, unitary_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Calculate the number of unitary matrices required to match the size of 'H'\n",
    "        n_us = round(math.log2(H_size) / math.log2(unitary_size))\n",
    "        self.n_us = n_us\n",
    "\n",
    "        # Initialize the given number of random unitary matrices\n",
    "        self.us = nn.ParameterList([nn.Parameter(random_unitary_matrix(unitary_size, device), requires_grad=True) for _ in range(n_us)])\n",
    "\n",
    "    def forward(self):\n",
    "        # Calculate U as the Kronecker product of all the unitary matrices in 'us'\n",
    "        U = self.us[0]\n",
    "        U = torch.kron(U, self.us[1])\n",
    "        U = torch.kron(U, self.us[2])\n",
    "        U = torch.kron(U, self.us[3])\n",
    "        # for i in range(4):\n",
    "        #     U = torch.kron(U, self.us[i+1])\n",
    "        # for u in self.us[1:]:\n",
    "        #     U = torch.kron(U, u)\n",
    "        return U\n",
    "\n",
    "def custom_loss(U, H):\n",
    "    # Calculate U @ H @ U.T\n",
    "    A = torch.matmul(U, torch.matmul(H, U.T))\n",
    "\n",
    "    # Calculate the absolute value of the result\n",
    "    a = torch.max(torch.diag(A)) * torch.eye(A.shape[0], device=device)\n",
    "    result_abs = -torch.abs(A - a) + a\n",
    "\n",
    "    # Calculate the minimum eigenvalue\n",
    "    E = torch.linalg.eigvalsh(result_abs)\n",
    "    z = torch.exp(-E * 1).sum()\n",
    "    return torch.log(z)\n",
    "\n",
    "p = dict(\n",
    "    Jx=1,\n",
    "    Jy=1,\n",
    "    Jz=1,\n",
    "    hx=0,\n",
    "    hz=0,\n",
    ")\n",
    "H = KH.system([2, 2], \"3site\", p)\n",
    "\n",
    "# Create an instance of the CustomModel class\n",
    "model = CustomModel(H.shape[0], 8).to(device)\n",
    "compiled_model = torch.compile(model, dynamic = False, fullgraph=True)\n",
    "# Calculate the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "635093cb382d24e7bb09df67eef84e97b3e0429c00b0294b3c9882ac411b8a1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
